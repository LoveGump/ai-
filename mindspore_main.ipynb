{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \u76ee\u6807\u68c0\u6d4b\uff1a\u53e3\u7f69\u4f69\u6234\u68c0\u6d4b  \n", "\n", "<br>\n", "<hr>"]}, {"cell_type": "markdown", "metadata": {"toc-hr-collapsed": true}, "source": ["## 1.\u5b9e\u9a8c\u4ecb\u7ecd"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.1 \u5b9e\u9a8c\u80cc\u666f  \n", "\n", "\u4eca\u5e74\u4e00\u573a\u5e2d\u5377\u5168\u7403\u7684\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\u7ed9\u4eba\u4eec\u5e26\u6765\u4e86\u6c89\u91cd\u7684\u751f\u547d\u8d22\u4ea7\u7684\u635f\u5931\u3002  \n", "\u6709\u6548\u9632\u5fa1\u8fd9\u79cd\u4f20\u67d3\u75c5\u6bd2\u7684\u65b9\u6cd5\u5c31\u662f\u79ef\u6781\u4f69\u6234\u53e3\u7f69\u3002  \n", "\u6211\u56fd\u5bf9\u6b64\u4e5f\u91c7\u53d6\u4e86\u4e25\u8083\u7684\u63aa\u65bd\uff0c\u5728\u516c\u5171\u573a\u5408\u8981\u6c42\u4eba\u4eec\u5fc5\u987b\u4f69\u6234\u53e3\u7f69\u3002  \n", "\u5728\u672c\u6b21\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u8981\u5efa\u7acb\u4e00\u4e2a\u76ee\u6807\u68c0\u6d4b\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u8bc6\u522b\u56fe\u4e2d\u7684\u4eba\u662f\u5426\u4f69\u6234\u4e86\u53e3\u7f69\u3002"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.2 \u5b9e\u9a8c\u8981\u6c42\n", "\n", "1. \u5efa\u7acb\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u68c0\u6d4b\u51fa\u56fe\u4e2d\u7684\u4eba\u662f\u5426\u4f69\u6234\u4e86\u53e3\u7f69\uff0c\u5e76\u5c06\u5176\u5c3d\u53ef\u80fd\u8c03\u6574\u5230\u6700\u4f73\u72b6\u6001\u3002 \n", "2. \u5b66\u4e60OpenCV dnn\u7684\u4f7f\u7528\u65b9\u6cd5\uff0c\u4ee5\u53ca\u7ecf\u5178\u6a21\u578b MobileNetV2 \u7684\u7ed3\u6784\u3002\n", "3. \u5b66\u4e60\u8bad\u7ec3\u65f6\u7684\u65b9\u6cd5\u3002"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.3 \u5b9e\u9a8c\u73af\u5883\n", "\n", "\u53ef\u4ee5\u4f7f\u7528\u57fa\u4e8e Python \u7684 OpenCV \u3001PIL \u5e93\u8fdb\u884c\u56fe\u50cf\u76f8\u5173\u5904\u7406\uff0c\u4f7f\u7528 Numpy \u5e93\u8fdb\u884c\u76f8\u5173\u6570\u503c\u8fd0\u7b97\uff0c\u4f7f\u7528 MindSpore \u7b49\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\u6a21\u578b\u7b49\u3002\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.4 \u6ce8\u610f\u4e8b\u9879  \n", "+ Python \u4e0e Python Package \u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u53ef\u5728\u53f3\u4fa7 `API\u6587\u6863` \u4e2d\u67e5\u9605\u3002\n", "+ \u5f53\u53f3\u4e0a\u89d2\u7684\u300ePython 3\u300f\u957f\u65f6\u95f4\u6307\u793a\u4e3a\u8fd0\u884c\u4e2d\u7684\u65f6\u5019\uff0c\u9020\u6210\u4ee3\u7801\u65e0\u6cd5\u6267\u884c\u65f6\uff0c\u53ef\u4ee5\u91cd\u65b0\u542f\u52a8 Kernel \u89e3\u51b3\uff08\u5de6\u4e0a\u89d2\u300eKernel\u300f-\u300eRestart Kernel\u300f\uff09\u3002"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.5 \u53c2\u8003\u8d44\u6599\n", "+ \u8bba\u6587 Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks\uff1ahttps://kpzhang93.github.io/MTCNN_face_detection_alignment/\n", "+ OpenCV\uff1ahttps://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_tutorials.html\n", "+ PIL\uff1ahttps://pillow.readthedocs.io/en/stable/\n", "+ Numpy\uff1ahttps://www.numpy.org/\n", "+ Scikit-learn\uff1a https://scikit-learn.org/\n", "+ mindspore\uff1ahttps://www.mindspore.cn/tutorial/training/zh-CN/master/index.html"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.6 \u5b9e\u9a8c\u601d\u8def\n", "\n", "\u9488\u5bf9\u76ee\u6807\u68c0\u6d4b\u7684\u4efb\u52a1\uff0c\u53ef\u4ee5\u5206\u4e3a\u4e24\u4e2a\u90e8\u5206\uff1a\u76ee\u6807\u8bc6\u522b\u548c\u4f4d\u7f6e\u68c0\u6d4b\u3002  \n", "\u901a\u5e38\u60c5\u51b5\u4e0b\uff0c\u7279\u5f81\u63d0\u53d6\u9700\u8981\u7531\u7279\u6709\u7684\u7279\u5f81\u63d0\u53d6\u795e\u7ecf\u7f51\u7edc\u6765\u5b8c\u6210\uff0c\u5982 VGG\u3001MobileNet\u3001ResNet \u7b49\uff0c\u8fd9\u4e9b\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u5f80\u5f80\u88ab\u79f0\u4e3a Backbone \u3002\u800c\u5728 BackBone \u540e\u9762\u63a5\u5168\u8fde\u63a5\u5c42(FC)\u5c31\u53ef\u4ee5\u6267\u884c\u5206\u7c7b\u4efb\u52a1\u3002  \n", "\u4f46 FC \u5bf9\u76ee\u6807\u7684\u4f4d\u7f6e\u8bc6\u522b\u4e4f\u529b\u3002\u7ecf\u8fc7\u7b97\u6cd5\u7684\u53d1\u5c55\uff0c\u5f53\u524d\u4e3b\u8981\u4ee5\u7279\u5b9a\u7684\u529f\u80fd\u7f51\u7edc\u6765\u4ee3\u66ff FC \u7684\u4f5c\u7528\uff0c\u5982 Mask-Rcnn\u3001SSD\u3001YOLO \u7b49\u3002  \n", "\u6211\u4eec\u9009\u62e9\u5145\u5206\u4f7f\u7528\u5df2\u6709\u7684\u4eba\u8138\u68c0\u6d4b\u7684\u6a21\u578b\uff0c\u518d\u8bad\u7ec3\u4e00\u4e2a\u8bc6\u522b\u53e3\u7f69\u7684\u6a21\u578b\uff0c\u4ece\u800c\u63d0\u9ad8\u8bad\u7ec3\u7684\u5f00\u652f\u3001\u589e\u5f3a\u6a21\u578b\u7684\u51c6\u786e\u7387\u3002\n", "\n", "**\u5e38\u89c4\u76ee\u6807\u68c0\u6d4b\uff1a**  \n", "\n", "<img src=\"https://imgbed.momodel.cn/20200914162156.png\" width=500px/>\n", "\n", "\n", "\n", "**\u672c\u6b21\u6848\u4f8b\uff1a**   \n", "\n", "\n", "<img src=\"https://imgbed.momodel.cn/20200918102630.png\" width=500px/>\n", "\n", "<br>\n", "<br>"]}, {"cell_type": "markdown", "metadata": {"toc-hr-collapsed": false}, "source": ["## 2. OpenCV \u4eba\u8138\u68c0\u6d4b"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u6570\u636e\u4fe1\u606f\u5b58\u653e\u5728 `/datasets/5f680a696ec9b83bb0037081-momodel/data` \u6587\u4ef6\u5939\u4e0b\u3002    \n", "\u8be5\u6587\u4ef6\u5939\u4e3b\u8981\u6709\u6587\u4ef6\u5939 `image`\u3001\u6587\u4ef6 `train.txt` \u3001\u6587\u4ef6\u5939 `keras_model_data` \u548c\u6587\u4ef6\u5939 `mindspore_model_data`\u5171\u56db\u90e8\u5206\uff1a\n", "+ **image \u6587\u4ef6\u5939**\uff1a\u56fe\u7247\u5206\u6210\u4e24\u7c7b\uff0c\u6234\u53e3\u7f69\u7684\u548c\u6ca1\u6709\u6234\u53e3\u7f69\u7684  \n", "+ **train.txt**\uff1a  \u5b58\u653e\u7684\u662f image \u6587\u4ef6\u5939\u4e0b\u5bf9\u5e94\u56fe\u7247\u7684\u6807\u7b7e  \uff08keras \u6846\u67b6\u4e13\u7528\u6587\u4ef6\uff09\n", "+ **keras_model_data** \u6587\u4ef6\u5939\uff1a\u5b58\u653e keras \u6846\u67b6\u76f8\u5173\u9884\u8bad\u7ec3\u597d\u7684\u6a21\u578b \uff08keras \u6846\u67b6\u4e13\u7528\u6587\u4ef6\u5939\uff09\n", "+ **mindspore_model_data** \u6587\u4ef6\u5939\uff1a\u5b58\u653e mindspore \u6846\u67b6\u76f8\u5173\u9884\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff08mindspore \u6846\u67b6\u4e13\u7528\u6587\u4ef6\uff09\n", "\n", "opencv \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u5728\u6570\u636e\u96c6 **mindspore_model_data/opencv_dnn** \u6587\u4ef6\u5939\u4e2d"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "select": true}, "outputs": [], "source": ["import os\n", "# \u6570\u636e\u96c6\u8def\u5f84\n", "basic_path = \"./datasets/5f680a696ec9b83bb0037081-momodel/data/\"\n", "# opencv \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u5728\u6570\u636e\u96c6 mindspore_model_data/opencv_dnn \u6587\u4ef6\u5939\u4e2d\n", "opencv_dnn_path = basic_path + 'mindspore_model_data/opencv_dnn'\n", "print(opencv_dnn_path)\n", "# \u67e5\u770b\u6587\u4ef6\u5939\u91cc\u9762\u6587\u4ef6\n", "os.listdir(opencv_dnn_path)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u53d1\u73b0\u8be5\u6587\u4ef6\u5939\u4e0b\u6709\u6211\u4eec\u9700\u8981\u7684\u8def\u5f84\uff0c\u6240\u4ee5\n", "+ **\u4f9d\u8d56\u6587\u4ef6\u5939\u7684\u8def\u5f84\u4e3a opencv_dnn_path** =           \n", "`/datasets/5f680a696ec9b83bb0037081-momodel/data/mindspore_model_data/opencv_dnn`\n", "\n", "+ **deploy.prototxt \u6587\u4ef6\u7684\u8def\u5f84**\uff1a       \n", "`opencv_dnn_path + '/' + 'deploy.prototxt'`\n", "+ **res10_300x300_ssd_iter_140000_fp16.caffemodel \u6587\u4ef6\u7684\u8def\u5f84**\uff1a        \n", "`opencv_dnn_path + '/' + 'res10_300x300_ssd_iter_140000_fp16.caffemodel'`"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "select": true}, "outputs": [], "source": ["import os\n", "import numpy as np\n", "import cv2\n", "import matplotlib.pyplot as plt\n", "\n", "class FaceDet():\n", "    def __init__(self):\n", "        self.opencv_dnn_path = 'datasets/5f680a696ec9b83bb0037081-momodel/data/mindspore_model_data/opencv_dnn/'\n", "        self.threshold = 0.15\n", "        self.caffe_model = self.opencv_dnn_path + \"deploy.prototxt\"\n", "        self.caffe_param = self.opencv_dnn_path + \"res10_300x300_ssd_iter_140000_fp16.caffemodel\"\n", "\n", "    def draw_detections(self, image, detections):\n", "        h, w, c = image.shape\n", "        for i in range(0, detections.shape[2]):\n", "            confidence = detections[0, 0, i, 2]\n", "            if confidence > self.threshold:\n", "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n", "                (startX, startY, endX, endY) = box.astype(\"int\")\n", "                text = \"{:.2f}%\".format(confidence * 100)\n", "                y = startY - 10 if startY - 10 > 10 else startY + 10\n", "                cv2.rectangle(image, (startX, startY), (endX, endY),\n", "                              (0, 255, 0), 1)\n", "                cv2.putText(image, text, (startX, y),\n", "                            cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n", "        return image\n", "\n", "    def detect(self, image):\n", "        net = cv2.dnn.readNetFromCaffe(self.caffe_model, self.caffe_param)\n", "\n", "        # def blobFromImage(image, scalefactor=None, size=None, mean=None, swapRB=None, crop=None, ddepth=None)\n", "        # image\uff1a\u8f93\u5165\u56fe\u50cf\n", "        # mean\uff1a\u5bf9\u6bcf\u4e2a\u901a\u9053\u50cf\u7d20\u503c\u51cf\u53bb\u5bf9\u5e94\u7684\u5747\u503c\uff0c\u8fd9\u91cc\u7528(104.0, 177.0, 123.0)\uff0c\u548c\u6a21\u578b\u8bad\u7ec3\u65f6\u7684\u503c\u4e00\u81f4\n", "        # scalefactor\uff1a\u5bf9\u50cf\u7d20\u503c\u7684\u7f29\u653e\u6bd4\u4f8b\n", "        # size\uff1a\u6a21\u578b\u8f93\u5165\u56fe\u7247\u7684\u5c3a\u5bf8\n", "        # swapRB\uff1aOpenCV\u9ed8\u8ba4\u7684\u56fe\u7247\u901a\u9053\u987a\u5e8f\u662fBGR\uff0c\u5982\u679c\u9700\u8981\u4ea4\u6362R\u548cG\uff0c\u5219\u8bbe\u4e3aTrue\n", "        # crop: \u8c03\u6574\u56fe\u7247\u5927\u5c0f\u540e\uff0c\u662f\u5426\u88c1\u526a\n", "        blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300), (104.0, 177.0, 123.0), False, False)\n", "        net.setInput(blob)\n", "        detections = net.forward()\n", "        return detections\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u9a8c\u8bc1\u4e00\u4e0b\u4eba\u8138\u68c0\u6d4b\u7684\u6548\u679c\uff0c\u5176\u4e2d\u4eba\u8138\u6846\u4e0a\u65b9\u7684`xx%`\u4e3a\u7f6e\u4fe1\u5ea6"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["img = cv2.imread(\"test.jpg\")\n", "detect = FaceDet()\n", "detections = detect.detect(img)\n", "drawed_img = detect.draw_detections(img, detections)\n", "\n", "# OpenCV reads image to BGR format. Transform images before showing it.\n", "drawed_img = cv2.cvtColor(drawed_img, cv2.COLOR_BGR2RGB)\n", "plt.figure(figsize = (8,8))\n", "plt.imshow(drawed_img)\n"]}, {"cell_type": "markdown", "metadata": {"toc-hr-collapsed": false}, "source": ["## 3 \u53e3\u7f69\u8bc6\u522b"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u5bfc\u5165\u6807\u51c6\u5e93\u3001\u7b2c\u4e09\u65b9\u5e93\uff0c\u5df2\u53caMindSpore\u7684\u6a21\u5757\u3002"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "select": true}, "outputs": [], "source": ["import math\n", "import os\n", "import numpy as np\n", "import cv2\n", "import matplotlib.pyplot as plt\n", "from easydict import EasyDict\n", "from PIL import Image\n", "\n", "from mindspore import context\n", "from mindspore import nn\n", "from mindspore import Tensor\n", "from mindspore.train.model import Model\n", "from mindspore.train.serialization import load_checkpoint\n", "from mindspore.train.callback import Callback\n", "from mindspore.train.callback import LossMonitor\n", "from mindspore.train.callback import ModelCheckpoint\n", "from mindspore.train.callback import CheckpointConfig\n", "\n", "# \u6a21\u578b\u5b9a\u4e49\u811a\u672c\u4ee5\u53ca\u6570\u636e\u5904\u7406\u811a\u672c\n", "from mindspore_py.mobilenetV2 import MobileNetV2Backbone\n", "from mindspore_py.mobilenetV2 import MobileNetV2Head\n", "from mindspore_py.mobilenetV2 import mobilenet_v2\n", "from mindspore_py.dataset import create_dataset\n", "# Log Level = Error\n", "os.environ['GLOG_v'] = '3'\n", "# \u8bbe\u7f6e\u91c7\u7528\u56fe\u6a21\u5f0f\u6267\u884c\uff0c\u8bbe\u5907\u4e3aCPU/GPU\n", "context.set_context(mode=context.GRAPH_MODE, device_target=\"CPU\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.1 \u6570\u636e\u96c6\u4ecb\u7ecd\n", "\n", "\u6570\u636e\u4fe1\u606f\u5b58\u653e\u5728 `/datasets/5f680a696ec9b83bb0037081-momodel/data/image` \u6587\u4ef6\u5939\u4e0b\u3002              \n", "\u6536\u96c6\u7684\u56fe\u7247\u5206\u6210 mask \u548c nomask \u4e24\u7c7b\uff0c\u6234\u53e3\u7f69\u7684\u548c\u6ca1\u6709\u6234\u53e3\u7f69\u7684\u3002        \n", "\u73b0\u5728\u6211\u4eec\u5c1d\u8bd5\u8bfb\u53d6\u6570\u636e\u96c6\u4e2d\u7684\u4e00\u5f20\u6234\u53e3\u7f69\u7684\u56fe\u7247\u5e76\u663e\u793a\u56fe\u7247\u540d\u79f0\u3002  \n", "\u73b0\u5728\u6211\u4eec\u5c1d\u8bd5\u8bfb\u53d6\u6570\u636e\u96c6\u4e2d\u6234\u53e3\u7f69\u7684\u56fe\u7247\u53ca\u5176\u540d\u79f0\uff0c\u4ee5\u4e0b\u662f\u8bad\u7ec3\u96c6\u4e2d\u7684\u6b63\u6837\u672c\uff1a"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mask_num = 4\n", "fig = plt.figure(figsize = (15,15))\n", "basic_path = \"./datasets/5f680a696ec9b83bb0037081-momodel/data/\"\n", "for i in range(mask_num):\n", "    sub_img = cv2.imread(basic_path + \"/image/mask/mask_\" + str(i + 101) + \".jpg\")\n", "    sub_img = cv2.cvtColor(sub_img, cv2.COLOR_RGB2BGR)\n", "    ax = fig.add_subplot(4, 4, (i + 1))\n", "    ax.set_xticks([])\n", "    ax.set_yticks([])\n", "    ax.set_title(\"mask_\" + str(i + 1))\n", "    ax.imshow(sub_img)\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u4ee5\u4e0b\u662f\u8bad\u7ec3\u96c6\u4e2d\u7684\u8d1f\u6837\u672c\uff1a"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["nomask_num = 4\n", "fig1 = plt.figure(figsize=(15, 15))\n", "for i in range(nomask_num):\n", "    sub_img = cv2.imread(basic_path + \"/image/nomask/nomask_\" + str(i + 130) + \".jpg\")\n", "    sub_img = cv2.cvtColor(sub_img, cv2.COLOR_RGB2BGR)\n", "    ax = fig1.add_subplot(4, 4, (i + 1))\n", "    ax.set_xticks([])\n", "    ax.set_yticks([])\n", "    ax.set_title(\"nomask_\" + str(i + 1))\n", "    ax.imshow(sub_img)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.2 \u6a21\u578b\u8bad\u7ec3Tips"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u914d\u7f6e\u540e\u7eed\u8bad\u7ec3\u3001\u9a8c\u8bc1\u3001\u63a8\u7406\u7528\u5230\u7684\u53c2\u6570\u3002"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "select": true}, "outputs": [], "source": ["basic_path = \"./datasets/5f680a696ec9b83bb0037081-momodel/data/\"\n", "config = EasyDict({\n", "    \"num_classes\": 2,\n", "    \"image_height\": 224,\n", "    \"image_width\": 224,\n", "    \"data_split\": [0.9, 0.1],\n", "    \"backbone_out_channels\":1280,\n", "    \"batch_size\": 16,\n", "    \"eval_batch_size\": 8,\n", "    \"epochs\": 3,\n", "    \"lr_max\": 0.01,\n", "    \"momentum\": 0.9,\n", "    \"weight_decay\": 1e-4,\n", "    \"save_checkpoint\": True,\n", "    \"save_checkpoint_epochs\": 1,\n", "    \"save_checkpoint_path\": \"./results\",\n", "    \"dataset_path\": basic_path + \"image\",\n", "    \"pretrained_ckpt\": basic_path + \"mindspore_model_data/mobilenetV2-200_1067.ckpt\"\n", "})\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 3.2.1 \u52a8\u6001\u5b66\u4e60\u7387\n", "\n", "\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u8bad\u7ec3\u65f6\u91c7\u7528\u9759\u6001\u5b66\u4e60\u7387\uff0c\u59820.01\u3002\u968f\u7740\u8bad\u7ec3\u6b65\u6570\u7684\u589e\u52a0\uff0c\u6a21\u578b\u9010\u6e10\u8d8b\u4e8e\u6536\u655b\uff0c\u5bf9\u6743\u91cd\u53c2\u6570\u7684\u66f4\u65b0\u5e45\u5ea6\u5e94\u8be5\u9010\u6e10\u964d\u4f4e\uff0c\u4ee5\u51cf\u5c0f\u6a21\u578b\u8bad\u7ec3\u540e\u671f\u7684\u6296\u52a8\u3002\u6240\u4ee5\uff0c\u6a21\u578b\u8bad\u7ec3\u65f6\u53ef\u4ee5\u91c7\u7528\u52a8\u6001\u4e0b\u964d\u7684\u5b66\u4e60\u7387\uff0c\u5e38\u89c1\u7684\u5b66\u4e60\u7387\u4e0b\u964d\u7b56\u7565\u6709\uff1a\n", "\n", "- polynomial decay/square decay\n", "- cosine decay\n", "- exponential decay\n", "- stage decay\n", "\n", "\u8fd9\u91cc\u5b9e\u73b0 cosine decay \u4e0b\u964d\u7b56\u7565\u3002"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "select": true}, "outputs": [], "source": ["def cosine_decay(total_steps, lr_init=0.0, lr_end=0.0, lr_max=0.1, warmup_steps=0):\n", "    \"\"\"\n", "    Applies cosine decay to generate learning rate array.\n", "\n", "    Args:\n", "       total_steps(int): all steps in training.\n", "       lr_init(float): init learning rate.\n", "       lr_end(float): end learning rate\n", "       lr_max(float): max learning rate.\n", "       warmup_steps(int): all steps in warmup epochs.\n", "\n", "    Returns:\n", "       list, learning rate array.\n", "    \"\"\"\n", "    lr_init, lr_end, lr_max = float(lr_init), float(lr_end), float(lr_max)\n", "    decay_steps = total_steps - warmup_steps\n", "    lr_all_steps = []\n", "    inc_per_step = (lr_max - lr_init) / warmup_steps if warmup_steps else 0\n", "    for i in range(total_steps):\n", "        if i < warmup_steps:\n", "            lr = lr_init + lr_inc * (i + 1)\n", "        else:\n", "            cosine_decay = 0.5 * (1 + math.cos(math.pi * (i - warmup_steps) / decay_steps))\n", "            lr = (lr_max - lr_end) * cosine_decay + lr_end\n", "        lr_all_steps.append(lr)\n", "\n", "    return lr_all_steps\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 3.2.2 \u8fb9\u8bad\u7ec3\u8fb9\u9a8c\u8bc1\n", "\n", "\u5728\u9762\u5bf9\u590d\u6742\u7f51\u7edc\u65f6\uff0c\u5f80\u5f80\u9700\u8981\u8fdb\u884c\u51e0\u5341\u751a\u81f3\u51e0\u767e\u6b21\u7684epoch\u8bad\u7ec3\u3002\u5728\u8bad\u7ec3\u4e4b\u524d\uff0c\u5f88\u96be\u638c\u63e1\u5728\u8bad\u7ec3\u5230\u7b2c\u51e0\u4e2aepoch\u65f6\uff0c\u6a21\u578b\u7684\u7cbe\u5ea6\u80fd\u8fbe\u5230\u6ee1\u8db3\u8981\u6c42\u7684\u7a0b\u5ea6\uff0c\u6240\u4ee5\u7ecf\u5e38\u4f1a\u91c7\u7528\u4e00\u8fb9\u8bad\u7ec3\u7684\u540c\u65f6\uff0c\u5728\u76f8\u9694\u56fa\u5b9aepoch\u7684\u4f4d\u7f6e\u5bf9\u6a21\u578b\u8fdb\u884c\u7cbe\u5ea6\u9a8c\u8bc1\uff0c\u5e76\u4fdd\u5b58\u76f8\u5e94\u7684\u6a21\u578b\uff0c\u7b49\u8bad\u7ec3\u5b8c\u6bd5\u540e\uff0c\u901a\u8fc7\u67e5\u770b\u5bf9\u5e94\u6a21\u578b\u7cbe\u5ea6\u7684\u53d8\u5316\u5c31\u80fd\u8fc5\u901f\u5730\u6311\u9009\u51fa\u76f8\u5bf9\u6700\u4f18\u7684\u6a21\u578b\u3002\u6d41\u7a0b\u5982\u4e0b\uff1a\n", "\n", "- \u5b9a\u4e49\u56de\u8c03\u51fd\u6570EvalCallback\uff0c\u5b9e\u73b0\u540c\u6b65\u8fdb\u884c\u8bad\u7ec3\u548c\u9a8c\u8bc1\u3002\n", "- \u5b9a\u4e49\u8bad\u7ec3\u7f51\u7edc\u5e76\u6267\u884c\u3002\n", "- \u5c06\u4e0d\u540cepoch\u4e0b\u7684\u6a21\u578b\u7cbe\u5ea6\u7ed8\u5236\u51fa\u6298\u7ebf\u56fe\u5e76\u6311\u9009\u6700\u4f18\u6a21\u578bCheckpoint\u3002\n", "\n", "\u5f53\u6211\u4eec\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc\u7684\u65f6\u5019\u901a\u5e38\u5e0c\u671b\u80fd\u83b7\u5f97\u6700\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002\u4f46\u662f\u6df1\u5ea6\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc\u5f88\u5bb9\u6613\u8fc7\u62df\u5408\u3002\u5f53\u7f51\u7edc\u5728\u8bad\u7ec3\u96c6\u4e0a\u8868\u73b0\u8d8a\u6765\u8d8a\u597d\uff0c\u9519\u8bef\u7387\u8d8a\u6765\u8d8a\u4f4e\u7684\u65f6\u5019\uff0c\u5c31\u6781\u6709\u53ef\u80fd\u51fa\u73b0\u4e86\u8fc7\u62df\u5408\u3002\u6211\u4eec\u53ef\u4ee5\u8bbe\u8ba1\u4e00\u79cd\u65e9\u505c\u6cd5\uff0c\u6bd4\u5982\u9a8c\u8bc1\u7cbe\u5ea6\u8fde\u7eed5\u6b21\u4e0d\u5728\u4e0a\u5347\u5c31\u505c\u6b62\u8bad\u7ec3\uff0c\u8fd9\u6837\u80fd\u907f\u514d\u7ee7\u7eed\u8bad\u7ec3\u5bfc\u81f4\u8fc7\u62df\u5408\u7684\u95ee\u9898\u3002"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "select": true}, "outputs": [], "source": ["class EvalCallback(Callback):\n", "    def __init__(self, model, eval_dataset, history, eval_epochs=1):\n", "        self.model = model\n", "        self.eval_dataset = eval_dataset\n", "        self.eval_epochs = eval_epochs\n", "        self.history = history\n", "        self.acc_max = 0\n", "        # acc\u8fde\u7eed5\u6b21<=\u8fc7\u7a0b\u4e2d\u7684\u6700\u5927\u503c\uff0c\u5219\u505c\u6b62\u8bad\u7ec3\n", "        self.count_max = 5\n", "        self.count = 0\n", "\n", "    def epoch_begin(self, run_context):\n", "        self.losses = []\n", "\n", "    def step_end(self, run_context):\n", "        cb_param = run_context.original_args()\n", "        loss = cb_param.net_outputs\n", "        self.losses.append(loss.asnumpy())\n", "\n", "    def epoch_end(self, run_context):\n", "        cb_param = run_context.original_args()\n", "        cur_epoch = cb_param.cur_epoch_num\n", "        train_loss = np.mean(self.losses)\n", "\n", "        if cur_epoch % self.eval_epochs == 0:\n", "            metric = self.model.eval(self.eval_dataset, dataset_sink_mode=False)\n", "            self.history[\"epoch\"].append(cur_epoch)\n", "            self.history[\"eval_acc\"].append(metric[\"acc\"])\n", "            self.history[\"eval_loss\"].append(metric[\"loss\"])\n", "            self.history[\"train_loss\"].append(train_loss)\n", "            if self.acc_max < metric[\"acc\"]:\n", "                self.count = 0\n", "                self.acc_max = metric[\"acc\"]\n", "            else:\n", "                self.count += 1\n", "                if self.count == self.count_max:\n", "                    run_context.request_stop()\n", "            print(\"epoch: %d, train_loss: %f, eval_loss: %f, eval_acc: %f\" %(cur_epoch, train_loss, metric[\"loss\"], metric[\"acc\"]))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.3 \u6a21\u578b\u8bad\u7ec3\n", "\n", "\u5728\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u53ef\u4ee5\u6dfb\u52a0\u68c0\u67e5\u70b9\uff08Checkpoint\uff09\u7528\u4e8e\u4fdd\u5b58\u6a21\u578b\u7684\u53c2\u6570\uff0c\u4ee5\u4fbf\u8fdb\u884c\u63a8\u7406\u53ca\u4e2d\u65ad\u540e\u518d\u8bad\u7ec3\u4f7f\u7528\u3002\u4f7f\u7528\u573a\u666f\u5982\u4e0b\uff1a\n", "\n", "- \u8bad\u7ec3\u540e\u63a8\u7406\u573a\u666f\n", "    - \u6a21\u578b\u8bad\u7ec3\u5b8c\u6bd5\u540e\u4fdd\u5b58\u6a21\u578b\u7684\u53c2\u6570\uff0c\u7528\u4e8e\u63a8\u7406\u6216\u9884\u6d4b\u64cd\u4f5c\u3002\n", "    - \u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7\u5b9e\u65f6\u9a8c\u8bc1\u7cbe\u5ea6\uff0c\u628a\u7cbe\u5ea6\u6700\u9ad8\u7684\u6a21\u578b\u53c2\u6570\u4fdd\u5b58\u4e0b\u6765\uff0c\u7528\u4e8e\u9884\u6d4b\u64cd\u4f5c\u3002\n", "- \u518d\u8bad\u7ec3\u573a\u666f\n", "    - \u8fdb\u884c\u957f\u65f6\u95f4\u8bad\u7ec3\u4efb\u52a1\u65f6\uff0c\u4fdd\u5b58\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684Checkpoint\u6587\u4ef6\uff0c\u9632\u6b62\u4efb\u52a1\u5f02\u5e38\u9000\u51fa\u540e\u4ece\u521d\u59cb\u72b6\u6001\u5f00\u59cb\u8bad\u7ec3\u3002\n", "    - Fine-tuning\uff08\u5fae\u8c03\uff09\u573a\u666f\uff0c\u5373\u8bad\u7ec3\u4e00\u4e2a\u6a21\u578b\u5e76\u4fdd\u5b58\u53c2\u6570\uff0c\u57fa\u4e8e\u8be5\u6a21\u578b\uff0c\u9762\u5411\u7b2c\u4e8c\u4e2a\u7c7b\u4f3c\u4efb\u52a1\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3002\n", "\n", "\u8fd9\u91cc\u52a0\u8f7d ImageNet \u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u7684 MobileNetv2 \u8fdb\u884c Fine-tuning\uff0c**\u53ea\u8bad\u7ec3\u6700\u540e\u4fee\u6539\u7684 FC \u5c42**\uff0c\u5e76\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fdd\u5b58 Checkpoint\u3002"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "select": true}, "outputs": [], "source": ["# \u8fd0\u884c\u8be5 cell \u4ee3\u7801\u8bad\u7ec3\u6a21\u578b\u65f6\uff0c\u8bf7\u6e05\u7406 results \u6587\u4ef6\u5939\u4e2d mindspore \u6846\u67b6\u4e4b\u524d\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff0c\u5426\u5219 model_path \u683c\u5f0f\u4f1a\u53d1\u751f\u4e00\u5b9a\u7684\u53d8\u5316\uff0c\u9020\u6210\u63a8\u7406\u65f6\u53ef\u80fd\u627e\u4e0d\u5230\u6a21\u578b\u7684\u62a5\u9519\n", "def train():\n", "    train_dataset, eval_dataset = create_dataset(dataset_path=config.dataset_path, config=config)\n", "    step_size = train_dataset.get_dataset_size()\n", "\n", "    backbone = MobileNetV2Backbone() #last_channel=config.backbone_out_channels\n", "    # Freeze parameters of backbone. You can comment these two lines.\n", "    for param in backbone.get_parameters():\n", "       param.requires_grad = False\n", "    # load parameters from pretrained model\n", "    load_checkpoint(config.pretrained_ckpt, backbone)\n", "\n", "    # head = MobileNetV2Head(num_classes=config.num_classes, last_channel=config.backbone_out_channels)\n", "    head = MobileNetV2Head(input_channel=backbone.out_channels, num_classes=config.num_classes)\n", "    network = mobilenet_v2(backbone, head)\n", "\n", "    # define loss, optimizer, and model\n", "    loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n", "    lrs = cosine_decay(config.epochs * step_size, lr_max=config.lr_max)\n", "    opt = nn.Momentum(network.trainable_params(), lrs, config.momentum, config.weight_decay)\n", "    model = Model(network, loss, opt, metrics={'acc', 'loss'})\n", "\n", "    history = {'epoch': [], 'train_loss': [], 'eval_loss': [], 'eval_acc': []}\n", "    eval_cb = EvalCallback(model, eval_dataset, history)\n", "    cb = [eval_cb]\n", "    if config.save_checkpoint:\n", "        ckpt_cfg = CheckpointConfig(save_checkpoint_steps=config.save_checkpoint_epochs * step_size, keep_checkpoint_max=config.epochs)\n", "        ckpt_cb = ModelCheckpoint(prefix=\"mobilenetv2_mask\", directory=config.save_checkpoint_path, config=ckpt_cfg)\n", "        cb.append(ckpt_cb)\n", "    model.train(config.epochs, train_dataset, callbacks=cb, dataset_sink_mode=False)\n", "\n", "    return history\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u5c06\u4e0d\u540c epoch \u4e0b\u7684\u6a21\u578b\u7cbe\u5ea6\u7ed8\u5236\u51fa\u6298\u7ebf\u56fe\u5e76\u6311\u9009\u6700\u4f18\u6a21\u578b Checkpoint\u3002"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "select": true}, "outputs": [], "source": ["history = train()\n", "\n", "plt.plot(history['epoch'], history['train_loss'], label='train_loss')\n", "plt.plot(history['epoch'], history['eval_loss'], 'r', label='val_loss')\n", "plt.legend()\n", "plt.show()\n", "\n", "plt.plot(history['epoch'], history['eval_acc'], 'r', label = 'val_acc')\n", "plt.legend()\n", "plt.show()\n", "\n", "model_path = 'results/mobilenetv2_mask-%d_39.ckpt' % (np.argmax(history['eval_acc']) + 1) # \u6311\u9009\u51fa\u6700\u4f18\u6a21\u578bCheckpoint\n", "print(\"the path of best model checkpoint is :\", model_path)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.4 \u6a21\u578b\u63a8\u7406\n", "\n", "\u52a0\u8f7d\u6a21\u578b Checkpoint \u8fdb\u884c\u63a8\u7406\u3002           \n", "\u4f7f\u7528 load_checkpoint \u63a5\u53e3\u52a0\u8f7d\u6570\u636e\u65f6\uff0c\u9700\u8981\u628a\u6570\u636e\u4f20\u5165\u7ed9\u539f\u59cb\u7f51\u7edc\uff0c\u800c\u4e0d\u80fd\u4f20\u9012\u7ed9\u5e26\u6709\u4f18\u5316\u5668\u548c\u635f\u5931\u51fd\u6570\u7684\u8bad\u7ec3\u7f51\u7edc\u3002"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "select": true}, "outputs": [], "source": ["def image_process(image):\n", "    \"\"\"Precess one image per time.\n", "\n", "    Args:\n", "        image: shape (H, W, C)\n", "    \"\"\"\n", "    mean=[0.485*255, 0.456*255, 0.406*255]\n", "    std=[0.229*255, 0.224*255, 0.225*255]\n", "    image = (np.array(image) - mean) / std\n", "    image = image.transpose((2,0,1))\n", "    img_tensor = Tensor(np.array([image], np.float32))\n", "    return img_tensor\n", "\n", "def infer_one(network, image_path):\n", "    image = Image.open(image_path).resize((config.image_height, config.image_width))\n", "    logits = network(image_process(image))\n", "    pred = np.argmax(logits.asnumpy(), axis=1)[0]\n", "    print(\"\u56fe\u7247\u8def\u5f84\uff1a\", image_path, \"\u56fe\u7247\u9884\u6d4b\u7c7b\u522b\",pred)\n", "\n", "def infer(basic_path, model_path):\n", "    backbone = MobileNetV2Backbone(last_channel=config.backbone_out_channels)\n", "    head = MobileNetV2Head(input_channel=backbone.out_channels, num_classes=config.num_classes)\n", "    network = mobilenet_v2(backbone, head)\n", "    load_checkpoint(model_path, network)\n", "    for i in range(250, 258):\n", "        infer_one(network, basic_path + 'image/mask/mask_%s.jpg' % i)\n", "    for i in range(371, 378):\n", "        infer_one(network, basic_path + 'image/nomask/nomask_%s.jpg' % i)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["infer(basic_path, model_path)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.5 \u53e3\u7f69\u8bc6\u522b"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "select": true}, "outputs": [], "source": ["class MaskRec():\n", "    def __init__(self, model_path):\n", "        self.face_det = FaceDet()\n", "        self.mask_model_input_size = (160, 160)\n", "        self.class_names = ['YES', 'NO']\n", "        # \u521d\u59cb\u5316MobileNetv2\n", "        backbone = MobileNetV2Backbone(last_channel=config.backbone_out_channels)\n", "        head = MobileNetV2Head(input_channel=backbone.out_channels, num_classes=config.num_classes)\n", "        self.mask_model = mobilenet_v2(backbone, head)\n", "        load_checkpoint(model_path, self.mask_model)\n", "\n", "    def to_small_square(self, startX, startY, endX, endY):\n", "        w = endX - startX\n", "        h = endY - startY\n", "        l = min(w, h)\n", "\n", "        startX = int(startX + (w - l) / 2)\n", "        endX = startX + l\n", "        startY = int(startY + (h - l) / 2)\n", "        endY = startY + l\n", "        return startX, startY, endX, endY\n", "\n", "    def recognize(self, image):\n", "        # \u4eba\u8138\u68c0\u6d4b\n", "        detections = self.face_det.detect(image)\n", "        h, w, c = image.shape\n", "        predict_labels = []\n", "        for i in range(0, detections.shape[2]):\n", "            confidence = detections[0, 0, i, 2]\n", "            if confidence > self.face_det.threshold:\n", "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n", "                startX, startY, endX, endY = box.astype(\"int\")\n", "                # \u622a\u53d6\u56fe\u50cf\n", "                startX, startY, endX, endY = self.to_small_square(startX, startY, endX, endY)\n", "                crop_img = image[startY:endY, startX:endX]\n", "                # \u56fe\u50cf\u9884\u5904\u7406, mask_model accept image with RGB\n", "                resized = cv2.resize(crop_img, (config.image_height, config.image_width))\n", "                img_tensor = image_process(cv2.cvtColor(crop_img, cv2.COLOR_BGR2RGB))\n", "                # \u9884\u6d4b\n", "                logits = self.mask_model(img_tensor)\n", "                predict_label = np.argmax(logits.asnumpy(), axis=1)[0]\n", "                predict_labels.append(predict_label)\n", "                # \u753b\u56fe\n", "                y = startY - 10 if startY - 10 > 10 else startY + 10\n", "                cv2.rectangle(image, (startX, startY), (endX, endY),\n", "                              (0, 255, 0), 1)\n", "                cv2.putText(image, self.class_names[predict_label], (startX, y),\n", "                            cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n", "        return image, len(predict_labels), predict_labels.count(0)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["img = cv2.imread(\"./test1.jpg\")\n", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n", "\n", "detect = MaskRec(model_path)\n", "img, all_num, mask_num = detect.recognize(img)\n", "\n", "# \u5c55\u793a\u56fe\u7247\u53e3\u7f69\u8bc6\u522b\u7ed3\u679c\n", "fig = plt.figure(figsize=(8, 8))\n", "ax1 = fig.add_subplot(111)\n", "ax1.set_xticks([])\n", "ax1.set_yticks([])\n", "ax1.set_title('test_mask')\n", "ax1.imshow(img)\n", "print(\"\u56fe\u4e2d\u7684\u4eba\u6570\u6709\uff1a\" + str(all_num) + \"\u4e2a\")\n", "print(\"\u6234\u53e3\u7f69\u7684\u4eba\u6570\u6709\uff1a\" + str(mask_num) + \"\u4e2a\")\n"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "select": true}, "source": ["## 4 \u4f5c\u4e1a\n", "**\u4f5c\u4e1a\u8981\u6c42\u53ca\u6ce8\u610f\u4e8b\u9879**\uff1a    \n", "\n", "1.\u4f7f\u7528\u4e0a\u8ff0\u5b66\u5230\u7684\u65b9\u6cd5\uff0c\u8bad\u7ec3\u81ea\u5df1\u7684\u53e3\u7f69\u8bc6\u522b\u6a21\u578b\uff0c\u5c3d\u53ef\u80fd\u63d0\u9ad8\u51c6\u786e\u5ea6\u3002\u5c06\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u4fdd\u5b58\u5728 results \u6587\u4ef6\u5939\u4e0b\u3002             \n", "2.\u70b9\u51fb\u5de6\u4fa7\u680f\u63d0\u4ea4\u4f5c\u4e1a\u540e\u70b9\u51fb\u3010\u751f\u6210\u6587\u4ef6\u3011\u5219\u9700\u8981\u52fe\u9009\u4e0e\u9884\u6d4b predict() \u51fd\u6570\u7684 cell\u76f8\u5173\u7684\u5176\u5b83cell \uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u6210\u4e3a main.py \u6587\u4ef6\u3002                       \n", "3.\u8bf7\u5bfc\u5165\u5fc5\u8981\u7684\u5305\u548c\u7b2c\u4e09\u65b9\u5e93\u4ee5\u53ca\u8be5\u6a21\u578b\u6240\u4f9d\u8d56\u7684 py \u6587\u4ef6 (\u5305\u62ec\u6b64\u6587\u4ef6\u4e2d\u66fe\u7ecf\u5bfc\u5165\u8fc7\u7684)\u3002             \n", "4.\u8bf7\u52a0\u8f7d\u4f60\u8ba4\u4e3a\u8bad\u7ec3\u6700\u4f73\u7684\u6a21\u578b\uff0c\u5373\u8bf7\u6309\u8981\u6c42\u586b\u5199\u6a21\u578b\u8def\u5f84\u3002              \n", "5.predict() \u51fd\u6570\u7684\u8f93\u5165\u8f93\u51fa\u53ca\u51fd\u6570\u540d\u79f0\u8bf7\u4e0d\u8981\u6539\u52a8\u3002\n", "\n", "\n", "===========================================  **\u6a21\u578b\u9884\u6d4b\u4ee3\u7801\u7b54\u9898\u533a\u57df**  ===========================================  \n", "\u5728\u4e0b\u65b9\u7684\u4ee3\u7801\u5757\u4e2d\u7f16\u5199 **\u6a21\u578b\u9884\u6d4b** \u90e8\u5206\u7684\u4ee3\u7801\uff0c\u8bf7\u52ff\u5728\u522b\u7684\u4f4d\u7f6e\u4f5c\u7b54"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "select": true}, "outputs": [], "source": ["# -------------------------- \u8bf7\u52a0\u8f7d\u60a8\u6700\u6ee1\u610f\u7684\u6a21\u578b -----------------------------\n", "# \u52a0\u8f7d\u6a21\u578b(\u8bf7\u52a0\u8f7d\u4f60\u8ba4\u4e3a\u7684\u6700\u4f73\u6a21\u578b)\n", "# \u52a0\u8f7d\u6a21\u578b,\u52a0\u8f7d\u8bf7\u6ce8\u610f model_path \u662f\u76f8\u5bf9\u8def\u5f84, \u4e0e\u5f53\u524d\u6587\u4ef6\u540c\u7ea7\u3002\n", "# \u5982\u679c\u4f60\u7684\u6a21\u578b\u662f\u5728 results \u6587\u4ef6\u5939\u4e0b\u7684 mobilenetv2_mask_1-1_39.ckpt \u6a21\u578b\uff0c\u5219 model_path = 'results/mobilenetv2_mask-1_39.ckpt'\n", "model_path = None\n", "# ---------------------------------------------------------------------------\n", "\n", "def predict(img):\n", "    \"\"\"\n", "    \u52a0\u8f7d\u6a21\u578b\u548c\u6a21\u578b\u9884\u6d4b\n", "    :param img: cv2.imread \u56fe\u50cf\n", "    :return: \u9884\u6d4b\u7684\u56fe\u7247\u4e2d\u7684\u603b\u4eba\u6570\u3001\u5176\u4e2d\u4f69\u6234\u53e3\u7f69\u7684\u4eba\u6570\n", "    \"\"\"\n", "    # -------------------------- \u5b9e\u73b0\u6a21\u578b\u9884\u6d4b\u90e8\u5206\u7684\u4ee3\u7801 ---------------------------\n", "\n", "    detect = MaskRec(model_path)\n", "    img, all_num, mask_num = detect.recognize(img)\n", "\n", "    # -------------------------------------------------------------------------\n", "\n", "    return all_num,mask_num\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u8f93\u5165\u56fe\u7247\u8def\u5f84\u548c\u540d\u79f0\n", "img = cv2.imread(\"test1.jpg\")\n", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n", "all_num,mask_num = predict(img)\n", "# \u6253\u5370\u9884\u6d4b\u8be5\u5f20\u56fe\u7247\u4e2d\u603b\u4eba\u6570\u4ee5\u53ca\u6234\u53e3\u7f69\u7684\u4eba\u6570\n", "print(all_num, mask_num)\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.5"}}, "nbformat": 4, "nbformat_minor": 4}