{"cells": [{"cell_type": "markdown", "metadata": {"toc-hr-collapsed": false}, "source": ["# \u76ee\u6807\u68c0\u6d4b\uff1a\u53e3\u7f69\u4f69\u6234\u68c0\u6d4b  \n", "\n", "<br>\n", "<hr>"]}, {"cell_type": "markdown", "metadata": {"toc-hr-collapsed": true}, "source": ["## 1.\u5b9e\u9a8c\u4ecb\u7ecd"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.1 \u5b9e\u9a8c\u80cc\u666f  \n", "\n", "\u4eca\u5e74\u4e00\u573a\u5e2d\u5377\u5168\u7403\u7684\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\u7ed9\u4eba\u4eec\u5e26\u6765\u4e86\u6c89\u91cd\u7684\u751f\u547d\u8d22\u4ea7\u7684\u635f\u5931\u3002  \n", "\u6709\u6548\u9632\u5fa1\u8fd9\u79cd\u4f20\u67d3\u75c5\u6bd2\u7684\u65b9\u6cd5\u5c31\u662f\u79ef\u6781\u4f69\u6234\u53e3\u7f69\u3002  \n", "\u6211\u56fd\u5bf9\u6b64\u4e5f\u91c7\u53d6\u4e86\u4e25\u8083\u7684\u63aa\u65bd\uff0c\u5728\u516c\u5171\u573a\u5408\u8981\u6c42\u4eba\u4eec\u5fc5\u987b\u4f69\u6234\u53e3\u7f69\u3002  \n", "\u5728\u672c\u6b21\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u8981\u5efa\u7acb\u4e00\u4e2a\u76ee\u6807\u68c0\u6d4b\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u8bc6\u522b\u56fe\u4e2d\u7684\u4eba\u662f\u5426\u4f69\u6234\u4e86\u53e3\u7f69\u3002"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.2 \u5b9e\u9a8c\u8981\u6c42\n", "\n", "1\uff09\u5efa\u7acb\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u68c0\u6d4b\u51fa\u56fe\u4e2d\u7684\u4eba\u662f\u5426\u4f69\u6234\u4e86\u53e3\u7f69\uff0c\u5e76\u5c06\u5176\u5c3d\u53ef\u80fd\u8c03\u6574\u5230\u6700\u4f73\u72b6\u6001\u3002  \n", "2\uff09\u5b66\u4e60\u7ecf\u5178\u7684\u6a21\u578b MTCNN \u548c MobileNet \u7684\u7ed3\u6784\u3002  \n", "3\uff09\u5b66\u4e60\u8bad\u7ec3\u65f6\u7684\u65b9\u6cd5\u3002  \n", "<br>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.3 \u5b9e\u9a8c\u73af\u5883\n", "\n", "\u53ef\u4ee5\u4f7f\u7528\u57fa\u4e8e Python \u7684 OpenCV \u3001PIL \u5e93\u8fdb\u884c\u56fe\u50cf\u76f8\u5173\u5904\u7406\uff0c\u4f7f\u7528 Numpy \u5e93\u8fdb\u884c\u76f8\u5173\u6570\u503c\u8fd0\u7b97\uff0c\u4f7f\u7528 Keras \u7b49\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\u6a21\u578b\u7b49\u3002\n", "<br>\n", "<br>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.4 \u6ce8\u610f\u4e8b\u9879  \n", "+ Python \u4e0e Python Package \u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u53ef\u5728\u53f3\u4fa7 `API\u6587\u6863` \u4e2d\u67e5\u9605\u3002\n", "+ \u5f53\u53f3\u4e0a\u89d2\u7684\u300ePython 3\u300f\u957f\u65f6\u95f4\u6307\u793a\u4e3a\u8fd0\u884c\u4e2d\u7684\u65f6\u5019\uff0c\u9020\u6210\u4ee3\u7801\u65e0\u6cd5\u6267\u884c\u65f6\uff0c\u53ef\u4ee5\u91cd\u65b0\u542f\u52a8 Kernel \u89e3\u51b3\uff08\u5de6\u4e0a\u89d2\u300eKernel\u300f-\u300eRestart Kernel\u300f\uff09\u3002"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.5 \u53c2\u8003\u8d44\u6599\n", "+ \u8bba\u6587 Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks\uff1ahttps://kpzhang93.github.io/MTCNN_face_detection_alignment/\n", "+ OpenCV\uff1ahttps://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_tutorials.html\n", "+ PIL\uff1ahttps://pillow.readthedocs.io/en/stable/\n", "+ Numpy\uff1ahttps://www.numpy.org/\n", "+ Scikit-learn\uff1a https://scikit-learn.org/\n", "+ tensorflow\uff1ahttps://www.tensorflow.org/api_docs/python/tf?hl=zh-cn\n", "+ keras\uff1ahttps://keras.io/zh/"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.6 \u5b9e\u9a8c\u601d\u8def\n", "\n", "\u9488\u5bf9\u76ee\u6807\u68c0\u6d4b\u7684\u4efb\u52a1\uff0c\u53ef\u4ee5\u5206\u4e3a\u4e24\u4e2a\u90e8\u5206\uff1a\u76ee\u6807\u8bc6\u522b\u548c\u4f4d\u7f6e\u68c0\u6d4b\u3002  \n", "\u901a\u5e38\u60c5\u51b5\u4e0b\uff0c\u7279\u5f81\u63d0\u53d6\u9700\u8981\u7531\u7279\u6709\u7684\u7279\u5f81\u63d0\u53d6\u795e\u7ecf\u7f51\u7edc\u6765\u5b8c\u6210\uff0c\u5982 VGG\u3001MobileNet\u3001ResNet \u7b49\uff0c\u8fd9\u4e9b\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u5f80\u5f80\u88ab\u79f0\u4e3a Backbone \u3002\u800c\u5728 BackBone \u540e\u9762\u63a5\u5168\u8fde\u63a5\u5c42(FC)\u5c31\u53ef\u4ee5\u6267\u884c\u5206\u7c7b\u4efb\u52a1\u3002  \n", "\u4f46 FC \u5bf9\u76ee\u6807\u7684\u4f4d\u7f6e\u8bc6\u522b\u4e4f\u529b\u3002\u7ecf\u8fc7\u7b97\u6cd5\u7684\u53d1\u5c55\uff0c\u5f53\u524d\u4e3b\u8981\u4ee5\u7279\u5b9a\u7684\u529f\u80fd\u7f51\u7edc\u6765\u4ee3\u66ff FC \u7684\u4f5c\u7528\uff0c\u5982 Mask-Rcnn\u3001SSD\u3001YOLO \u7b49\u3002  \n", "\u6211\u4eec\u9009\u62e9\u5145\u5206\u4f7f\u7528\u5df2\u6709\u7684\u4eba\u8138\u68c0\u6d4b\u7684\u6a21\u578b\uff0c\u518d\u8bad\u7ec3\u4e00\u4e2a\u8bc6\u522b\u53e3\u7f69\u7684\u6a21\u578b\uff0c\u4ece\u800c\u63d0\u9ad8\u8bad\u7ec3\u7684\u5f00\u652f\u3001\u589e\u5f3a\u6a21\u578b\u7684\u51c6\u786e\u7387\u3002\n", "\n", "**\u5e38\u89c4\u76ee\u6807\u68c0\u6d4b\uff1a**  \n", "\n", "<img src=\"https://imgbed.momodel.cn/20200914162156.png\" width=500px/>\n", "\n", "\n", "\n", "**\u672c\u6b21\u6848\u4f8b\uff1a**   \n", "\n", "\n", "<img src=\"https://imgbed.momodel.cn/20200918102630.png\" width=500px/>\n", "\n", "<br>\n", "<br>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2.\u6570\u636e\u96c6\u4ecb\u7ecd"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.1 \u5bfc\u5165 Python \u7b2c\u4e09\u65b9\u5e93\uff08\u5305\uff09"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "select": true}, "outputs": [], "source": ["import warnings\n", "# \u5ffd\u89c6\u8b66\u544a\n", "warnings.filterwarnings('ignore')\n", "import os\n", "import matplotlib\n", "import cv2 as cv\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n", "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n", "from tensorflow.keras import backend as K\n", "from tensorflow.keras.optimizers import Adam\n", "from keras.utils import np_utils,get_file\n", "\n", "K.image_data_format() == 'channels_last'\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.2 \u5bfc\u5165\u5df2\u7ecf\u5199\u597d\u7684 Python \u6587\u4ef6"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "select": true}, "outputs": [], "source": ["from keras_py.utils import get_random_data\n", "from keras_py.face_rec import mask_rec\n", "from keras_py.face_rec import face_rec\n", "from keras_py.mobileNet import MobileNet\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.3 \u6570\u636e\u96c6\u4ecb\u7ecd\n", "\n", "\u6570\u636e\u4fe1\u606f\u5b58\u653e\u5728 `/datasets/5f680a696ec9b83bb0037081-momodel/data` \u6587\u4ef6\u5939\u4e0b\u3002    \n", "\u8be5\u6587\u4ef6\u5939\u4e3b\u8981\u6709\u6587\u4ef6\u5939 `image`\u3001\u6587\u4ef6 `train.txt` \u3001\u6587\u4ef6\u5939 `keras_model_data` \u548c\u6587\u4ef6\u5939 `mindspore_model_data`\u5171\u56db\u90e8\u5206\uff1a\n", "+ **image \u6587\u4ef6\u5939**\uff1a\u56fe\u7247\u5206\u6210\u4e24\u7c7b\uff0c\u6234\u53e3\u7f69\u7684\u548c\u6ca1\u6709\u6234\u53e3\u7f69\u7684  \n", "+ **train.txt**\uff1a  \u5b58\u653e\u7684\u662f image \u6587\u4ef6\u5939\u4e0b\u5bf9\u5e94\u56fe\u7247\u7684\u6807\u7b7e \uff08keras \u6846\u67b6\u4e13\u7528\u6587\u4ef6\uff09\n", "+ **keras_model_data** \u6587\u4ef6\u5939\uff1a\u5b58\u653e keras \u6846\u67b6\u76f8\u5173\u9884\u8bad\u7ec3\u597d\u7684\u6a21\u578b \uff08keras \u6846\u67b6\u4e13\u7528\u6587\u4ef6\u5939\uff09\n", "+ **mindspore_model_data** \u6587\u4ef6\u5939\uff1a\u5b58\u653e mindspore \u6846\u67b6\u76f8\u5173\u9884\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff08mindspore \u6846\u67b6\u4e13\u7528\u6587\u4ef6\u5939\uff09"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u6570\u636e\u96c6\u8def\u5f84\n", "basic_path = \"./datasets/5f680a696ec9b83bb0037081-momodel/data/\"\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u73b0\u5728\u6211\u4eec\u5c1d\u8bd5\u8bfb\u53d6\u6570\u636e\u96c6\u4e2d\u6234\u53e3\u7f69\u7684\u56fe\u7247\u53ca\u5176\u540d\u79f0"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u4ee5\u4e0b\u662f\u8bad\u7ec3\u96c6\u4e2d\u7684\u6b63\u6837\u672c\uff1a"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mask_num = 4\n", "fig = plt.figure(figsize=(15, 15))\n", "for i in range(mask_num):\n", "    sub_img = cv.imread(basic_path + \"/image/mask/mask_\" + str(i + 101) + \".jpg\")\n", "    sub_img = cv.cvtColor(sub_img, cv.COLOR_RGB2BGR)\n", "    ax = fig.add_subplot(4, 4, (i + 1))\n", "    ax.set_xticks([])\n", "    ax.set_yticks([])\n", "    ax.set_title(\"mask_\" + str(i + 1))\n", "    ax.imshow(sub_img)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u4ee5\u4e0b\u662f\u8bad\u7ec3\u96c6\u4e2d\u7684\u8d1f\u6837\u672c\uff1a"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["nomask_num = 4\n", "fig1 = plt.figure(figsize=(15, 15))\n", "for i in range(nomask_num):\n", "    sub_img = cv.imread(basic_path + \"/image/nomask/nomask_\" + str(i + 130) + \".jpg\")\n", "    sub_img = cv.cvtColor(sub_img, cv.COLOR_RGB2BGR)\n", "    ax = fig1.add_subplot(4, 4, (i + 1))\n", "    ax.set_xticks([])\n", "    ax.set_yticks([])\n", "    ax.set_title(\"nomask_\" + str(i + 1))\n", "    ax.imshow(sub_img)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.4 \u8c03\u6574\u56fe\u7247\u5c3a\u5bf8"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def letterbox_image(image, size):\n", "    \"\"\"\n", "    \u8c03\u6574\u56fe\u7247\u5c3a\u5bf8\n", "    :param image: \u7528\u4e8e\u8bad\u7ec3\u7684\u56fe\u7247\n", "    :param size: \u9700\u8981\u8c03\u6574\u5230\u7f51\u7edc\u8f93\u5165\u7684\u56fe\u7247\u5c3a\u5bf8\n", "    :return: \u8fd4\u56de\u7ecf\u8fc7\u8c03\u6574\u7684\u56fe\u7247\n", "    \"\"\"\n", "    new_image = cv.resize(image, size, interpolation=cv.INTER_AREA)\n", "    return new_image\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u67e5\u770b\u56fe\u7247\u5c3a\u5bf8\u8c03\u6574\u524d\u540e\u7684\u5bf9\u6bd4"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["read_img = cv.imread(\"test1.jpg\")\n", "print(\"\u8c03\u6574\u524d\u56fe\u7247\u7684\u5c3a\u5bf8:\", read_img.shape)\n", "read_img = letterbox_image(image=read_img, size=(50, 50))\n", "print(\"\u8c03\u6574\u524d\u56fe\u7247\u7684\u5c3a\u5bf8:\", read_img.shape)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.5 \u5236\u4f5c\u8bad\u7ec3\u65f6\u6240\u9700\u7684\u6279\u91cf\u6570\u636e\u96c6"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u56fe\u7247\u751f\u6210\u5668 [ImageDataGenerator](https://keras.io/preprocessing/image/): keras.preprocessing.image \u6a21\u5757\u4e2d\u7684\u56fe\u7247\u751f\u6210\u5668\uff0c\u4e3b\u8981\u7528\u4ee5\u751f\u6210\u4e00\u4e2a batch \u7684\u56fe\u50cf\u6570\u636e\uff0c\u652f\u6301\u5b9e\u65f6\u6570\u636e\u63d0\u5347\u3002\u8bad\u7ec3\u65f6\u8be5\u51fd\u6570\u4f1a\u65e0\u9650\u751f\u6210\u6570\u636e\uff0c\u76f4\u5230\u8fbe\u5230\u89c4\u5b9a\u7684 epoch \u6b21\u6570\u4e3a\u6b62\u3002\u540c\u65f6\u4e5f\u53ef\u4ee5\u5728 batch \u4e2d\u5bf9\u6570\u636e\u8fdb\u884c\u589e\u5f3a\uff0c\u6269\u5145\u6570\u636e\u96c6\u5927\u5c0f\uff0c\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6bd4\u5982\u8fdb\u884c\u65cb\u8f6c\uff0c\u53d8\u5f62\uff0c\u5f52\u4e00\u5316\u7b49\u7b49\u3002\n", "    \n", "\u56fe\u7247\u751f\u6210\u5668\u7684\u4e3b\u8981\u65b9\u6cd5\uff1a\n", "+ fit(x, augment=False, rounds=1)\uff1a\u8ba1\u7b97\u4f9d\u8d56\u4e8e\u6570\u636e\u7684\u53d8\u6362\u6240\u9700\u8981\u7684\u7edf\u8ba1\u4fe1\u606f(\u5747\u503c\u65b9\u5dee\u7b49)\u3002  \n", "\n", "+ flow(self, X, y, batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix='', save_format='png')\uff1a\u63a5\u6536 Numpy \u6570\u7ec4\u548c\u6807\u7b7e\u4e3a\u53c2\u6570,\u751f\u6210\u7ecf\u8fc7\u6570\u636e\u63d0\u5347\u6216\u6807\u51c6\u5316\u540e\u7684 batch \u6570\u636e\uff0c\u5e76\u5728\u4e00\u4e2a\u65e0\u9650\u5faa\u73af\u4e2d\u4e0d\u65ad\u7684\u8fd4\u56de batch \u6570\u636e\u3002  \n", "\n", "\n", "+ flow_from_directory(directory): \u4ee5\u6587\u4ef6\u5939\u8def\u5f84\u4e3a\u53c2\u6570\uff0c\u4f1a\u4ece\u8def\u5f84\u63a8\u6d4b label\uff0c\u751f\u6210\u7ecf\u8fc7\u6570\u636e\u63d0\u5347/\u5f52\u4e00\u5316\u540e\u7684\u6570\u636e\uff0c\u5728\u4e00\u4e2a\u65e0\u9650\u5faa\u73af\u4e2d\u65e0\u9650\u4ea7\u751f batch \u6570\u636e\u3002\n", "\n", "\u82f1\u6587\u53c2\u8003\u94fe\u63a5\uff1ahttps://keras.io/preprocessing/image/  \n", "\u4e2d\u6587\u53c2\u8003\u94fe\u63a5\uff1ahttps://keras-cn.readthedocs.io/en/latest/preprocessing/image/\n", "\n", "\u4ee5\u4e0a\u53ea\u662f\u5bf9\u56fe\u7247\u751f\u6210\u5668\u8fdb\u884c\u7b80\u5355\u7684\u4ecb\u7ecd\uff0c\u8be6\u7ec6\u4fe1\u606f\u8bf7\u53c2\u8003\u4e2d\u82f1\u6587\u94fe\u63a5\u3002  \n", "\u6839\u636e\u4e0a\u9762\u7684\u4ecb\u7ecd\u548c\u6211\u4eec\u6570\u636e\u96c6\u7684\u7279\u6027\uff0c\u6211\u4eec\u4e3b\u8981\u8fd0\u7528 `ImageDataGenerator()` \u548c `flow_from_directory()` \u65b9\u6cd5\u3002\u6211\u4eec\u5c06\u6570\u636e\u5904\u7406\u8fc7\u7a0b\u5c01\u88c5\u6210\u4e3a\u4e00\u4e2a\u51fd\u6570\uff1a"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u5bfc\u5165\u56fe\u7247\u751f\u6210\u5668\n", "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n", "\n", "\n", "def processing_data(data_path, height, width, batch_size=32, test_split=0.1):\n", "    \"\"\"\n", "    \u6570\u636e\u5904\u7406\n", "    :param data_path: \u5e26\u6709\u5b50\u76ee\u5f55\u7684\u6570\u636e\u96c6\u8def\u5f84\n", "    :param height: \u56fe\u50cf\u5f62\u72b6\u7684\u884c\u6570\n", "    :param width: \u56fe\u50cf\u5f62\u72b6\u7684\u5217\u6570\n", "    :param batch_size: batch \u6570\u636e\u7684\u5927\u5c0f\uff0c\u6574\u6570\uff0c\u9ed8\u8ba432\u3002\n", "    :param test_split: \u5728 0 \u548c 1 \u4e4b\u95f4\u6d6e\u52a8\u3002\u7528\u4f5c\u6d4b\u8bd5\u96c6\u7684\u8bad\u7ec3\u6570\u636e\u7684\u6bd4\u4f8b\uff0c\u9ed8\u8ba40.1\u3002\n", "    :return: train_generator, test_generator: \u5904\u7406\u540e\u7684\u8bad\u7ec3\u96c6\u6570\u636e\u3001\u9a8c\u8bc1\u96c6\u6570\u636e\n", "    \"\"\"\n", "\n", "    train_data = ImageDataGenerator(\n", "            # \u5bf9\u56fe\u7247\u7684\u6bcf\u4e2a\u50cf\u7d20\u503c\u5747\u4e58\u4e0a\u8fd9\u4e2a\u653e\u7f29\u56e0\u5b50\uff0c\u628a\u50cf\u7d20\u503c\u653e\u7f29\u52300\u548c1\u4e4b\u95f4\u6709\u5229\u4e8e\u6a21\u578b\u7684\u6536\u655b\n", "            rescale=1. / 255,\n", "            # \u6d6e\u70b9\u6570\uff0c\u526a\u5207\u5f3a\u5ea6\uff08\u9006\u65f6\u9488\u65b9\u5411\u7684\u526a\u5207\u53d8\u6362\u89d2\u5ea6\uff09\n", "            shear_range=0.1,\n", "            # \u968f\u673a\u7f29\u653e\u7684\u5e45\u5ea6\uff0c\u82e5\u4e3a\u6d6e\u70b9\u6570\uff0c\u5219\u76f8\u5f53\u4e8e[lower,upper] = [1 - zoom_range, 1+zoom_range]\n", "            zoom_range=0.1,\n", "            # \u6d6e\u70b9\u6570\uff0c\u56fe\u7247\u5bbd\u5ea6\u7684\u67d0\u4e2a\u6bd4\u4f8b\uff0c\u6570\u636e\u63d0\u5347\u65f6\u56fe\u7247\u6c34\u5e73\u504f\u79fb\u7684\u5e45\u5ea6\n", "            width_shift_range=0.1,\n", "            # \u6d6e\u70b9\u6570\uff0c\u56fe\u7247\u9ad8\u5ea6\u7684\u67d0\u4e2a\u6bd4\u4f8b\uff0c\u6570\u636e\u63d0\u5347\u65f6\u56fe\u7247\u7ad6\u76f4\u504f\u79fb\u7684\u5e45\u5ea6\n", "            height_shift_range=0.1,\n", "            # \u5e03\u5c14\u503c\uff0c\u8fdb\u884c\u968f\u673a\u6c34\u5e73\u7ffb\u8f6c\n", "            horizontal_flip=True,\n", "            # \u5e03\u5c14\u503c\uff0c\u8fdb\u884c\u968f\u673a\u7ad6\u76f4\u7ffb\u8f6c\n", "            vertical_flip=True,\n", "            # \u5728 0 \u548c 1 \u4e4b\u95f4\u6d6e\u52a8\u3002\u7528\u4f5c\u9a8c\u8bc1\u96c6\u7684\u8bad\u7ec3\u6570\u636e\u7684\u6bd4\u4f8b\n", "            validation_split=test_split\n", "    )\n", "\n", "    # \u63a5\u4e0b\u6765\u751f\u6210\u6d4b\u8bd5\u96c6\uff0c\u53ef\u4ee5\u53c2\u8003\u8bad\u7ec3\u96c6\u7684\u5199\u6cd5\n", "    test_data = ImageDataGenerator(\n", "            rescale=1. / 255,\n", "            validation_split=test_split)\n", "\n", "    train_generator = train_data.flow_from_directory(\n", "            # \u63d0\u4f9b\u7684\u8def\u5f84\u4e0b\u9762\u9700\u8981\u6709\u5b50\u76ee\u5f55\n", "            data_path,\n", "            # \u6574\u6570\u5143\u7ec4 (height, width)\uff0c\u9ed8\u8ba4\uff1a(256, 256)\u3002 \u6240\u6709\u7684\u56fe\u50cf\u5c06\u88ab\u8c03\u6574\u5230\u7684\u5c3a\u5bf8\u3002\n", "            target_size=(height, width),\n", "            # \u4e00\u6279\u6570\u636e\u7684\u5927\u5c0f\n", "            batch_size=batch_size,\n", "            # \"categorical\", \"binary\", \"sparse\", \"input\" \u6216 None \u4e4b\u4e00\u3002\n", "            # \u9ed8\u8ba4\uff1a\"categorical\",\u8fd4\u56deone-hot \u7f16\u7801\u6807\u7b7e\u3002\n", "            class_mode='categorical',\n", "            # \u6570\u636e\u5b50\u96c6 (\"training\" \u6216 \"validation\")\n", "            subset='training',\n", "            seed=0)\n", "    test_generator = test_data.flow_from_directory(\n", "            data_path,\n", "            target_size=(height, width),\n", "            batch_size=batch_size,\n", "            class_mode='categorical',\n", "            subset='validation',\n", "            seed=0)\n", "\n", "    return train_generator, test_generator\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u6570\u636e\u8def\u5f84\n", "data_path = basic_path + 'image'\n", "\n", "# \u56fe\u50cf\u6570\u636e\u7684\u884c\u6570\u548c\u5217\u6570\n", "height, width = 160, 160\n", "\n", "# \u83b7\u53d6\u8bad\u7ec3\u6570\u636e\u548c\u9a8c\u8bc1\u6570\u636e\u96c6\n", "train_generator, test_generator = processing_data(data_path, height, width)\n", "\n", "# \u901a\u8fc7\u5c5e\u6027class_indices\u53ef\u83b7\u5f97\u6587\u4ef6\u5939\u540d\u4e0e\u7c7b\u7684\u5e8f\u53f7\u7684\u5bf9\u5e94\u5b57\u5178\u3002 (\u7c7b\u522b\u7684\u987a\u5e8f\u5c06\u6309\u7167\u5b57\u6bcd\u8868\u987a\u5e8f\u6620\u5c04\u5230\u6807\u7b7e\u503c)\u3002\n", "labels = train_generator.class_indices\n", "print(labels)\n", "\n", "# \u8f6c\u6362\u4e3a\u7c7b\u7684\u5e8f\u53f7\u4e0e\u6587\u4ef6\u5939\u540d\u5bf9\u5e94\u7684\u5b57\u5178\n", "labels = dict((v, k) for k, v in labels.items())\n", "print(labels)\n"]}, {"cell_type": "markdown", "metadata": {"toc-hr-collapsed": false}, "source": ["## 3. MTCNN\uff1a\u4eba\u8138\u68c0\u6d4b"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.1  MTCNN \u89e3\u8bfb\n", "\n", "\u53c2\u8003\u6587\u732e\uff1a\u300aJoint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks\u300b  \n", "\u6587\u732e\u4e0e\u4ee3\u7801\u5730\u5740\uff1ahttps://kpzhang93.github.io/MTCNN_face_detection_alignment/  \n", "  \n", "\u8bba\u6587\u7684\u4e3b\u8981\u8d21\u732e\uff1a  \n", "1\uff09**\u4e09\u9636\u6bb5\u7684\u7ea7\u8054\uff08cascaded\uff09\u67b6\u6784**  \n", "2\uff09**coarse-to-fine \u7684\u65b9\u5f0f**  \n", "3\uff09**new online hard sample mining \u7b56\u7565**  \n", "4\uff09**\u540c\u65f6\u8fdb\u884c\u4eba\u8138\u68c0\u6d4b\u548c\u4eba\u8138\u5bf9\u9f50**  \n", "5\uff09**state-of-the-art \u6027\u80fd**  \n", "\n", "<img src=\"https://imgbed.momodel.cn/20200918102724.png\"/>"]}, {"cell_type": "markdown", "metadata": {"toc-hr-collapsed": false}, "source": ["### 3.2 MTCNN \u7684\u4f7f\u7528\n", "\n", "\u8fd9\u91cc\u76f4\u63a5\u4f7f\u7528\u73b0\u6709\u7684\u8868\u73b0\u8f83\u597d\u7684 MTCNN \u7684\u4e09\u4e2a\u6743\u91cd\u6587\u4ef6\uff0c\u5b83\u4eec\u5df2\u7ecf\u4fdd\u5b58\u5728 `datasets/5f680a696ec9b83bb0037081-momodel/data/keras_model_data` \u6587\u4ef6\u5939\u4e0b\uff0c\u8def\u5f84\u5982\u4e0b\uff1a\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pnet_path = \"./datasets/5f680a696ec9b83bb0037081-momodel/data/keras_model_data/pnet.h5\"\n", "rnet_path = \"./datasets/5f680a696ec9b83bb0037081-momodel/data/keras_model_data/rnet.h5\"\n", "onet_path = \"./datasets/5f680a696ec9b83bb0037081-momodel/data/keras_model_data/onet.h5\"\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u901a\u8fc7\u642d\u5efa MTCNN \u7f51\u7edc\u5b9e\u73b0\u4eba\u8138\u68c0\u6d4b\uff08\u642d\u5efa\u6a21\u578bpy\u6587\u4ef6\u5728 keras_py \u6587\u4ef6\u5939\uff09 \u3002 \n", "+ keras_py/mtcnn.py  \u6587\u4ef6\u662f\u5728\u642d\u5efa MTCNN \u7f51\u7edc\u3002  \n", "+ keras_py/face_rec.py  \u6587\u4ef6\u662f\u5728\u7ed8\u5236\u4eba\u8138\u68c0\u6d4b\u7684\u77e9\u5f62\u6846\u3002"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u8bfb\u53d6\u6d4b\u8bd5\u56fe\u7247\n", "img = cv.imread(\"test.jpg\")\n", "# \u8f6c\u6362\u901a\u9053\n", "img = cv.cvtColor(img, cv.COLOR_RGB2BGR)\n", "# \u52a0\u8f7d\u6a21\u578b\u8fdb\u884c\u8bc6\u522b\u53e3\u7f69\u5e76\u7ed8\u5236\u65b9\u6846\n", "detect = face_rec(pnet_path,rnet_path,onet_path)\n", "detect.recognize(img)\n", "# \u5c55\u793a\u7ed3\u679c\n", "fig = plt.figure(figsize = (8,8))\n", "ax1 = fig.add_subplot(111)\n", "ax1.set_xticks([])\n", "ax1.set_yticks([])\n", "ax1.set_title('mask_1')\n", "ax1.imshow(img)\n"]}, {"cell_type": "markdown", "metadata": {"toc-hr-collapsed": false}, "source": ["## 4. \u53e3\u7f69\u8bc6\u522b\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 4.1 \u9884\u8bad\u7ec3\u6a21\u578b MobileNet"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u52a0\u8f7d MobileNet \u7684\u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd\n", "weights_path = basic_path + 'keras_model_data/mobilenet_1_0_224_tf_no_top.h5'\n", "# \u56fe\u50cf\u6570\u636e\u7684\u884c\u6570\u548c\u5217\u6570\n", "height, width = 160, 160\n", "model = MobileNet(input_shape=[height,width,3],classes=2)\n", "model.load_weights(weights_path,by_name=True)\n", "print('\u52a0\u8f7d\u5b8c\u6210...')\n"]}, {"cell_type": "markdown", "metadata": {"toc-hr-collapsed": false}, "source": ["### 4.2 \u51c6\u5907\u8bad\u7ec3\u6a21\u578b Tip"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### **4.2.1 \u52a0\u8f7d\u548c\u4fdd\u5b58\u6a21\u578b**(\u53ef\u8c03\u53c2\uff09\n", "\n", "\u4e3a\u4e86\u907f\u514d\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9047\u5230\u65ad\u7535\u7b49\u7a81\u53d1\u4e8b\u4ef6\uff0c\u5bfc\u81f4\u6a21\u578b\u8bad\u7ec3\u6210\u679c\u65e0\u6cd5\u4fdd\u5b58\u3002  \n", "\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 ModelCheckpoint \u89c4\u5b9a\u5728\u56fa\u5b9a\u8fed\u4ee3\u6b21\u6570\u540e\u4fdd\u5b58\u6a21\u578b\u3002  \n", "\u540c\u65f6\uff0c\u6211\u4eec\u8bbe\u7f6e\u5728\u4e0b\u4e00\u6b21\u91cd\u542f\u8bad\u7ec3\u65f6\uff0c\u4f1a\u68c0\u67e5\u662f\u5426\u6709\u4e0a\u6b21\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff0c\u5982\u679c\u6709\uff0c\u5c31\u5148\u52a0\u8f7d\u5df2\u6709\u7684\u6a21\u578b\u6743\u91cd\u3002  \n", "\u8fd9\u6837\u5c31\u53ef\u4ee5\u5728\u4e0a\u6b21\u8bad\u7ec3\u7684\u57fa\u7840\u4e0a\u7ee7\u7eed\u6a21\u578b\u7684\u8bad\u7ec3\u4e86\u3002"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def save_model(model, checkpoint_save_path, model_dir):\n", "    \"\"\"\n", "    \u4fdd\u5b58\u6a21\u578b\uff0c\u6bcf\u8fed\u4ee33\u6b21\u4fdd\u5b58\u4e00\u6b21\n", "    :param model: \u8bad\u7ec3\u7684\u6a21\u578b\n", "    :param checkpoint_save_path: \u52a0\u8f7d\u5386\u53f2\u6a21\u578b\n", "    :param model_dir:\n", "    :return:\n", "    \"\"\"\n", "    if os.path.exists(checkpoint_save_path):\n", "        print(\"\u6a21\u578b\u52a0\u8f7d\u4e2d\")\n", "        model.load_weights(checkpoint_save_path)\n", "        print(\"\u6a21\u578b\u52a0\u8f7d\u5b8c\u6bd5\")\n", "    checkpoint_period = ModelCheckpoint(\n", "        # \u6a21\u578b\u5b58\u50a8\u8def\u5f84\n", "        model_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n", "        # \u68c0\u6d4b\u7684\u6307\u6807\n", "        monitor='val_acc',\n", "        # \u2018auto\u2019\uff0c\u2018min\u2019\uff0c\u2018max\u2019\u4e2d\u9009\u62e9\n", "        mode='max',\n", "        # \u662f\u5426\u53ea\u5b58\u50a8\u6a21\u578b\u6743\u91cd\n", "        save_weights_only=False,\n", "        # \u662f\u5426\u53ea\u4fdd\u5b58\u6700\u4f18\u7684\u6a21\u578b\n", "        save_best_only=True,\n", "        # \u68c0\u6d4b\u7684\u8f6e\u6570\u662f\u6bcf\u96942\u8f6e\n", "        period=2\n", "    )\n", "    return checkpoint_period\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["checkpoint_save_path = \"./results/temp.h5\"\n", "model_dir = \"./results/\"\n", "checkpoint_period = save_model(model, checkpoint_save_path, model_dir)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### **4.2.2 \u624b\u52a8\u8c03\u6574\u5b66\u4e60\u7387**(\u53ef\u8c03\u53c2\uff09\n", "\n", "\u5b66\u4e60\u7387\u7684\u624b\u52a8\u8bbe\u7f6e\u53ef\u4ee5\u4f7f\u6a21\u578b\u8bad\u7ec3\u66f4\u52a0\u9ad8\u6548\u3002  \n", "\u8fd9\u91cc\u6211\u4eec\u8bbe\u7f6e\u5f53\u6a21\u578b\u5728\u4e24\u8f6e\u8fed\u4ee3\u540e\uff0c\u51c6\u786e\u7387\u6ca1\u6709\u4e0a\u5347\uff0c\u5c31\u8c03\u6574\u5b66\u4e60\u7387\u3002"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u5b66\u4e60\u7387\u4e0b\u964d\u7684\u65b9\u5f0f\uff0cacc\u4e09\u6b21\u4e0d\u4e0b\u964d\u5c31\u4e0b\u964d\u5b66\u4e60\u7387\u7ee7\u7eed\u8bad\u7ec3\n", "reduce_lr = ReduceLROnPlateau(\n", "                        monitor='acc',  # \u68c0\u6d4b\u7684\u6307\u6807\n", "                        factor=0.5,     # \u5f53acc\u4e0d\u4e0b\u964d\u65f6\u5c06\u5b66\u4e60\u7387\u4e0b\u8c03\u7684\u6bd4\u4f8b\n", "                        patience=2,     # \u68c0\u6d4b\u8f6e\u6570\u662f\u6bcf\u9694\u4e24\u8f6e\n", "                        verbose=2       # \u4fe1\u606f\u5c55\u793a\u6a21\u5f0f\n", "                    )\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### **4.2.3 \u65e9\u505c\u6cd5**(\u53ef\u8c03\u53c2\uff09\n", "\n", "\u5f53\u6211\u4eec\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc\u7684\u65f6\u5019\u901a\u5e38\u5e0c\u671b\u80fd\u83b7\u5f97\u6700\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002    \n", "\u4f46\u662f\u6240\u6709\u7684\u6807\u51c6\u6df1\u5ea6\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u5982\u5168\u8fde\u63a5\u591a\u5c42\u611f\u77e5\u673a\u90fd\u5f88\u5bb9\u6613\u8fc7\u62df\u5408\u3002    \n", "\u5f53\u7f51\u7edc\u5728\u8bad\u7ec3\u96c6\u4e0a\u8868\u73b0\u8d8a\u6765\u8d8a\u597d\uff0c\u9519\u8bef\u7387\u8d8a\u6765\u8d8a\u4f4e\u7684\u65f6\u5019\uff0c\u5c31\u6781\u6709\u53ef\u80fd\u51fa\u73b0\u4e86\u8fc7\u62df\u5408\u3002  \n", "\u65e9\u505c\u6cd5\u5c31\u662f\u5f53\u6211\u4eec\u5728\u68c0\u6d4b\u5230\u8fd9\u4e00\u8d8b\u52bf\u540e\uff0c\u5c31\u505c\u6b62\u8bad\u7ec3\uff0c\u8fd9\u6837\u80fd\u907f\u514d\u7ee7\u7eed\u8bad\u7ec3\u5bfc\u81f4\u8fc7\u62df\u5408\u7684\u95ee\u9898\u3002"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["early_stopping = EarlyStopping(\n", "                            monitor='val_loss',  # \u68c0\u6d4b\u7684\u6307\u6807\n", "                            min_delta=0,         # \u589e\u5927\u6216\u51cf\u5c0f\u7684\u9608\u503c\n", "                            patience=10,         # \u68c0\u6d4b\u7684\u8f6e\u6570\u9891\u7387\n", "                            verbose=1            # \u4fe1\u606f\u5c55\u793a\u7684\u6a21\u5f0f\n", "                        )\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 4.3 \u8bad\u7ec3\u6a21\u578b"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u4e00\u6b21\u7684\u8bad\u7ec3\u96c6\u5927\u5c0f\n", "batch_size = 8\n", "# \u56fe\u7247\u6570\u636e\u8def\u5f84\n", "data_path = basic_path + 'image'\n", "# \u56fe\u7247\u5904\u7406\n", "train_generator,test_generator = processing_data(data_path, height=160, width=160, batch_size=batch_size, test_split=0.1)\n", "# \u7f16\u8bd1\u6a21\u578b\n", "model.compile(loss='binary_crossentropy',  # \u4e8c\u5206\u7c7b\u635f\u5931\u51fd\u6570\n", "              optimizer=Adam(lr=0.1),            # \u4f18\u5316\u5668\n", "              metrics=['accuracy'])        # \u4f18\u5316\u76ee\u6807\n", "# \u8bad\u7ec3\u6a21\u578b\n", "history = model.fit(train_generator,\n", "                    epochs=3, # epochs: \u6574\u6570\uff0c\u6570\u636e\u7684\u8fed\u4ee3\u603b\u8f6e\u6570\u3002\n", "                    # \u4e00\u4e2aepoch\u5305\u542b\u7684\u6b65\u6570,\u901a\u5e38\u5e94\u8be5\u7b49\u4e8e\u4f60\u7684\u6570\u636e\u96c6\u7684\u6837\u672c\u6570\u91cf\u9664\u4ee5\u6279\u91cf\u5927\u5c0f\u3002\n", "                    steps_per_epoch=637 // batch_size,\n", "                    validation_data=test_generator,\n", "                    validation_steps=70 // batch_size,\n", "                    initial_epoch=0, # \u6574\u6570\u3002\u5f00\u59cb\u8bad\u7ec3\u7684\u8f6e\u6b21\uff08\u6709\u52a9\u4e8e\u6062\u590d\u4e4b\u524d\u7684\u8bad\u7ec3\uff09\u3002\n", "                    callbacks=[checkpoint_period, reduce_lr])\n", "# \u4fdd\u5b58\u6a21\u578b\n", "model.save_weights(model_dir + 'temp.h5')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 4.4 \u5c55\u793a\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(history.history['loss'],label = 'train_loss')\n", "plt.plot(history.history['val_loss'],'r',label = 'val_loss')\n", "plt.legend()\n", "plt.show()\n", "\n", "plt.plot(history.history['accuracy'],label = 'acc')\n", "plt.plot(history.history['val_accuracy'],'r',label = 'val_acc')\n", "plt.legend()\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 4.5 \u68c0\u6d4b\u56fe\u7247\u4e2d\u4eba\u6570\u53ca\u6234\u53e3\u7f69\u7684\u4eba\u6570"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import cv2 as cv\n", "# \u8bfb\u53d6\u56fe\u7247\n", "img = cv.imread(\"./test1.jpg\")\n", "img = cv.cvtColor(img, cv.COLOR_RGB2BGR)\n", "\n", "# \u6700\u4f73\u6a21\u578b\u8def\u5f84\n", "model_path = \"results/temp.h5\"\n", "\n", "# \u52a0\u8f7d\u8bad\u7ec3\u6a21\u578b\u5e76\u8fdb\u884c\u53e3\u7f69\u8bc6\u522b\n", "detect = mask_rec(model_path)\n", "img, all_num, mask_num = detect.recognize(img)\n", "\n", "# \u5c55\u793a\u56fe\u7247\u53e3\u7f69\u8bc6\u522b\u7ed3\u679c\n", "fig = plt.figure(figsize=(8, 8))\n", "ax1 = fig.add_subplot(111)\n", "ax1.set_xticks([])\n", "ax1.set_yticks([])\n", "ax1.set_title('test_mask')\n", "ax1.imshow(img)\n", "print(\"\u56fe\u4e2d\u7684\u4eba\u6570\u6709\uff1a\" + str(all_num) + \"\u4e2a\")\n", "print(\"\u6234\u53e3\u7f69\u7684\u4eba\u6570\u6709\uff1a\" + str(mask_num) + \"\u4e2a\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5.\u4f5c\u4e1a"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 5.1 \u8bad\u7ec3\u6a21\u578b"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u53e3\u7f69\u4f69\u6234\u68c0\u6d4b\u6a21\u578b\u8bad\u7ec3\u6d41\u7a0b, \u5305\u542b\u6570\u636e\u5904\u7406\u3001\u521b\u5efa\u6a21\u578b\u3001\u8bad\u7ec3\u6a21\u578b\u3001\u6a21\u578b\u4fdd\u5b58\u3001\u8bc4\u4ef7\u6a21\u578b\u7b49\u3002\u8bad\u7ec3\u6a21\u578b\u53ef\u4ee5\u53c2\u8003\u7b2c 4.3 \u90e8\u5206\u8bad\u7ec3\u6a21\u578b\u4ee3\u7801  \n", "\u5982\u679c\u5bf9\u8bad\u7ec3\u51fa\u6765\u7684\u6a21\u578b\u4e0d\u6ee1\u610f, \u4f60\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u6a21\u578b\u7684\u53c2\u6570\u7b49\u65b9\u6cd5\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b, \u76f4\u81f3\u8bad\u7ec3\u51fa\u4f60\u6ee1\u610f\u7684\u6a21\u578b\u3002  \n", "\u5982\u679c\u4f60\u5bf9\u81ea\u5df1\u8bad\u7ec3\u51fa\u6765\u7684\u6a21\u578b\u975e\u5e38\u6ee1\u610f, \u5219\u53ef\u4ee5\u63d0\u4ea4\u4f5c\u4e1a!  \n", "\n", "\u6ce8\u610f\uff1a\n", "\n", "1. \u4f60\u53ef\u4ee5\u5728\u6211\u4eec\u51c6\u597d\u7684\u63a5\u53e3\u4e2d\u5b9e\u73b0\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u82e5\u4f7f\u7528\u53ef\u4ee5\u4fee\u6539\u51fd\u6570\u63a5\u53e3\uff09\uff0c\u4e5f\u53ef\u4ee5\u81ea\u5df1\u5b9e\u73b0\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\n", "2. \u5199\u597d\u4ee3\u7801\u540e\u53ef\u4ee5\u5728 Py \u6587\u4ef6\u4e2d\u4f7f\u7528 [\u79bb\u7ebf\u4efb\u52a1](https://momodel.cn/docs/#/zh-cn/%E5%9C%A8GPU%E6%88%96CPU%E8%B5%84%E6%BA%90%E4%B8%8A%E8%AE%AD%E7%BB%83%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B) \u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3002\n", "3. **\u4f7f\u7528\u79bb\u7ebf\u8bad\u7ec3\u6a21\u578b\u5fc5\u987b\u4fdd\u5b58\u5728 results \u6587\u4ef6\u5939**\u3002    \n", "4. \u5c06\u81ea\u5df1\u8ba4\u4e3a\u6700\u4f73\u6a21\u578b\u4fdd\u5b58\u5728 result \u6587\u4ef6\u5939\uff0c\u5176\u4f59\u6a21\u578b\u5907\u4efd\u5728\u9879\u76ee\u4e2d\u5176\u5b83\u6587\u4ef6\u5939\uff0c\u65b9\u4fbf\u60a8\u52a0\u5feb\u6d4b\u8bd5\u901a\u8fc7\u3002\n"]}, {"cell_type": "markdown", "metadata": {"inputHidden": false}, "source": ["===========================================  \u5b9e\u73b0\u81ea\u5df1\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ee3\u7801\u7b54\u9898\u533a\u57df  ===========================================\n", "\n", "\u53cc\u51fb\u4e0b\u65b9\u533a\u57df\u5f00\u59cb\u7f16\u5199  **\u6570\u636e\u5904\u7406**\u3001**\u521b\u5efa\u6a21\u578b**\u3001**\u8bad\u7ec3\u6a21\u578b**\u3001**\u4fdd\u5b58\u6a21\u578b**  \u548c  **\u8bc4\u4f30\u6a21\u578b**  \u7b49\u90e8\u5206\u7684\u4ee3\u7801\uff0c\u8bf7\u52ff\u5728\u522b\u7684\u4f4d\u7f6e\u4f5c\u7b54"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 1.\u52a0\u8f7d\u6570\u636e\u5e76\u8fdb\u884c\u6570\u636e\u5904\u7406\n", "\n", "# 2.\u5982\u679c\u6709\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5219\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\uff1b\u5982\u679c\u6ca1\u6709\u5219\u4e0d\u9700\u8981\u52a0\u8f7d\n", "\n", "# 3.\u521b\u5efa\u6a21\u578b\u548c\u8bad\u7ec3\u6a21\u578b\uff0c\u8bad\u7ec3\u6a21\u578b\u65f6\u5c3d\u91cf\u5c06\u6a21\u578b\u4fdd\u5b58\u5728 results \u6587\u4ef6\u5939\n", "\n", "# 4.\u8bc4\u4f30\u6a21\u578b\uff0c\u5c06\u81ea\u5df1\u8ba4\u4e3a\u6700\u4f73\u6a21\u578b\u4fdd\u5b58\u5728 result \u6587\u4ef6\u5939\uff0c\u5176\u4f59\u6a21\u578b\u5907\u4efd\u5728\u9879\u76ee\u4e2d\u5176\u5b83\u6587\u4ef6\u5939\uff0c\u65b9\u4fbf\u60a8\u52a0\u5feb\u6d4b\u8bd5\u901a\u8fc7\u3002\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 5.2 \u63d0\u4ea4\u4f5c\u4e1a"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**\u4f5c\u4e1a\u8981\u6c42\u53ca\u6ce8\u610f\u4e8b\u9879**\uff1a    \n", "\n", "1.\u4f7f\u7528\u4e0a\u8ff0\u5b66\u5230\u7684\u65b9\u6cd5\uff0c\u8bad\u7ec3\u81ea\u5df1\u7684\u53e3\u7f69\u8bc6\u522b\u6a21\u578b\uff0c\u5c3d\u53ef\u80fd\u63d0\u9ad8\u51c6\u786e\u5ea6\u3002\u5c06\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u4fdd\u5b58\u5728 results \u6587\u4ef6\u5939\u4e0b\u3002             \n", "2.\u70b9\u51fb\u5de6\u4fa7\u680f\u63d0\u4ea4\u4f5c\u4e1a\u540e\u70b9\u51fb\u3010\u751f\u6210\u6587\u4ef6\u3011\u5219\u9700\u8981\u52fe\u9009\u4e0e\u9884\u6d4b predict() \u51fd\u6570\u7684 cell\u76f8\u5173\u7684\u5176\u5b83cell \uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u6210\u4e3a main.py \u6587\u4ef6\u3002                       \n", "3.\u8bf7\u5bfc\u5165\u5fc5\u8981\u7684\u5305\u548c\u7b2c\u4e09\u65b9\u5e93\u4ee5\u53ca\u8be5\u6a21\u578b\u6240\u4f9d\u8d56\u7684 py \u6587\u4ef6 (\u5305\u62ec\u6b64\u6587\u4ef6\u4e2d\u66fe\u7ecf\u5bfc\u5165\u8fc7\u7684)\u3002             \n", "4.\u8bf7\u52a0\u8f7d\u4f60\u8ba4\u4e3a\u8bad\u7ec3\u6700\u4f73\u7684\u6a21\u578b\uff0c\u5373\u8bf7\u6309\u8981\u6c42\u586b\u5199\u6a21\u578b\u8def\u5f84\u3002              \n", "5.predict() \u51fd\u6570\u7684\u8f93\u5165\u8f93\u51fa\u53ca\u51fd\u6570\u540d\u79f0\u8bf7\u4e0d\u8981\u6539\u52a8\u3002\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["===========================================  **\u6a21\u578b\u9884\u6d4b\u4ee3\u7801\u7b54\u9898\u533a\u57df**  ===========================================  \n", "\u5728\u4e0b\u65b9\u7684\u4ee3\u7801\u5757\u4e2d\u7f16\u5199 **\u6a21\u578b\u9884\u6d4b** \u90e8\u5206\u7684\u4ee3\u7801\uff0c\u8bf7\u52ff\u5728\u522b\u7684\u4f4d\u7f6e\u4f5c\u7b54"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "select": true}, "outputs": [], "source": ["from keras_py.utils import get_random_data\n", "from keras_py.face_rec import mask_rec\n", "from keras_py.face_rec import face_rec\n", "from keras_py.mobileNet import MobileNet\n", "from PIL import Image\n", "import cv2\n", "\n", "# -------------------------- \u8bf7\u52a0\u8f7d\u60a8\u6700\u6ee1\u610f\u7684\u6a21\u578b ---------------------------\n", "# \u52a0\u8f7d\u6a21\u578b(\u8bf7\u52a0\u8f7d\u4f60\u8ba4\u4e3a\u7684\u6700\u4f73\u6a21\u578b)\n", "# \u52a0\u8f7d\u6a21\u578b,\u52a0\u8f7d\u8bf7\u6ce8\u610f model_path \u662f\u76f8\u5bf9\u8def\u5f84, \u4e0e\u5f53\u524d\u6587\u4ef6\u540c\u7ea7\u3002\n", "# \u5982\u679c\u4f60\u7684\u6a21\u578b\u662f\u5728 results \u6587\u4ef6\u5939\u4e0b\u7684 dnn.h5 \u6a21\u578b\uff0c\u5219 model_path = 'results/temp.h5'\n", "model_path = None\n", "# ---------------------------------------------------------------------------\n", "\n", "def predict(img):\n", "    \"\"\"\n", "    \u52a0\u8f7d\u6a21\u578b\u548c\u6a21\u578b\u9884\u6d4b\n", "    :param img: cv2.imread \u56fe\u50cf\n", "    :return: \u9884\u6d4b\u7684\u56fe\u7247\u4e2d\u7684\u603b\u4eba\u6570\u3001\u5176\u4e2d\u4f69\u6234\u53e3\u7f69\u7684\u4eba\u6570\n", "    \"\"\"\n", "    # -------------------------- \u5b9e\u73b0\u6a21\u578b\u9884\u6d4b\u90e8\u5206\u7684\u4ee3\u7801 ---------------------------\n", "    # \u5c06 cv2.imread \u56fe\u50cf\u8f6c\u5316\u4e3a PIL.Image \u56fe\u50cf\uff0c\u7528\u6765\u517c\u5bb9\u6d4b\u8bd5\u8f93\u5165\u7684 cv2 \u8bfb\u53d6\u7684\u56fe\u50cf\uff08\u52ff\u5220\uff01\uff01\uff01\uff09\n", "    # cv2.imread \u8bfb\u53d6\u56fe\u50cf\u7684\u7c7b\u578b\u662f numpy.ndarray\n", "    # PIL.Image.open \u8bfb\u53d6\u56fe\u50cf\u7684\u7c7b\u578b\u662f PIL.JpegImagePlugin.JpegImageFile\n", "    if isinstance(img, np.ndarray):\n", "        # \u8f6c\u5316\u4e3a PIL.JpegImagePlugin.JpegImageFile \u7c7b\u578b\n", "        img = Image.fromarray(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n", "\n", "    detect = mask_rec(model_path)\n", "    img, all_num, mask_num = detect.recognize(img)\n", "\n", "    # -------------------------------------------------------------------------\n", "    return all_num,mask_num\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u8f93\u5165\u56fe\u7247\u8def\u5f84\u548c\u540d\u79f0\n", "img = cv.imread(\"test1.jpg\")\n", "img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n", "all_num,mask_num = predict(img)\n", "# \u6253\u5370\u9884\u6d4b\u8be5\u5f20\u56fe\u7247\u4e2d\u603b\u4eba\u6570\u4ee5\u53ca\u6234\u53e3\u7f69\u7684\u4eba\u6570\n", "print(all_num, mask_num)\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.5"}}, "nbformat": 4, "nbformat_minor": 4}